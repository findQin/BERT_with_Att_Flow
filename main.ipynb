{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this notebook we train our model: a BERT-like model with attention flow inspired by BiDAF."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Dependencies:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import collections, time, spacy, copy\n",
    "from layers.bert_plus_bidaf import BERT_plus_BiDAF\n",
    "from utils import data_processing\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "source": [
    "This part should be data loading and processing.\n",
    "\n",
    "Input: SQuAD dataset handler/url/json\n",
    "\n",
    "Output: processed dict/list/whatever: train_question, train_context, train_answer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "val_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "train_encodings =  data_processing.data_processing(train_url)\n",
    "val_encodings = data_processing.data_processing(val_url)"
   ]
  },
  {
   "source": [
    "Create a smaller dataset for debugging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in train_encodings.keys():\n",
    "    train_encodings[key] = train_encodings[key][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in val_encodings.keys():\n",
    "    val_encodings[key] = val_encodings[key][0:100]"
   ]
  },
  {
   "source": [
    "Templates for S/L to save time preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_encodings,r'D:\\OneDrive\\Courses\\ECS289 NLP\\train_encodings.pt')\n",
    "torch.save(val_encodings,r'D:\\OneDrive\\Courses\\ECS289 NLP\\val_encodings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = torch.load(r'D:\\OneDrive\\Courses\\ECS289 NLP\\train_encodings.pt')\n",
    "val_encodings = torch.load(r'D:\\OneDrive\\Courses\\ECS289 NLP\\val_encodings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,encodings):\n",
    "    self.encodings = encodings\n",
    "  def __getitem__(self,idx):\n",
    "    return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "source": [
    "This part should be model construction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_plus_BiDAF(if_extra_modeling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BERT_plus_BiDAF(\n",
       "  (bert_layer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attention_layer): AttFlow(\n",
       "    (weight): Linear(in_features=2304, out_features=1, bias=False)\n",
       "  )\n",
       "  (modeling_layer): LSTM(3072, 1536, num_layers=2)\n",
       "  (prediction_layer): PredictionLayer(\n",
       "    (pred_start): Linear(in_features=1536, out_features=1, bias=False)\n",
       "    (pred_end): Linear(in_features=1536, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "source": [
    "This part should be declaration of the optimizer and the loss function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameters to learn:\n\t bert_layer.embeddings.word_embeddings.weight\n\t bert_layer.embeddings.position_embeddings.weight\n\t bert_layer.embeddings.token_type_embeddings.weight\n\t bert_layer.embeddings.LayerNorm.weight\n\t bert_layer.embeddings.LayerNorm.bias\n\t bert_layer.encoder.layer.0.attention.self.query.weight\n\t bert_layer.encoder.layer.0.attention.self.query.bias\n\t bert_layer.encoder.layer.0.attention.self.key.weight\n\t bert_layer.encoder.layer.0.attention.self.key.bias\n\t bert_layer.encoder.layer.0.attention.self.value.weight\n\t bert_layer.encoder.layer.0.attention.self.value.bias\n\t bert_layer.encoder.layer.0.attention.output.dense.weight\n\t bert_layer.encoder.layer.0.attention.output.dense.bias\n\t bert_layer.encoder.layer.0.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.0.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.0.intermediate.dense.weight\n\t bert_layer.encoder.layer.0.intermediate.dense.bias\n\t bert_layer.encoder.layer.0.output.dense.weight\n\t bert_layer.encoder.layer.0.output.dense.bias\n\t bert_layer.encoder.layer.0.output.LayerNorm.weight\n\t bert_layer.encoder.layer.0.output.LayerNorm.bias\n\t bert_layer.encoder.layer.1.attention.self.query.weight\n\t bert_layer.encoder.layer.1.attention.self.query.bias\n\t bert_layer.encoder.layer.1.attention.self.key.weight\n\t bert_layer.encoder.layer.1.attention.self.key.bias\n\t bert_layer.encoder.layer.1.attention.self.value.weight\n\t bert_layer.encoder.layer.1.attention.self.value.bias\n\t bert_layer.encoder.layer.1.attention.output.dense.weight\n\t bert_layer.encoder.layer.1.attention.output.dense.bias\n\t bert_layer.encoder.layer.1.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.1.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.1.intermediate.dense.weight\n\t bert_layer.encoder.layer.1.intermediate.dense.bias\n\t bert_layer.encoder.layer.1.output.dense.weight\n\t bert_layer.encoder.layer.1.output.dense.bias\n\t bert_layer.encoder.layer.1.output.LayerNorm.weight\n\t bert_layer.encoder.layer.1.output.LayerNorm.bias\n\t bert_layer.encoder.layer.2.attention.self.query.weight\n\t bert_layer.encoder.layer.2.attention.self.query.bias\n\t bert_layer.encoder.layer.2.attention.self.key.weight\n\t bert_layer.encoder.layer.2.attention.self.key.bias\n\t bert_layer.encoder.layer.2.attention.self.value.weight\n\t bert_layer.encoder.layer.2.attention.self.value.bias\n\t bert_layer.encoder.layer.2.attention.output.dense.weight\n\t bert_layer.encoder.layer.2.attention.output.dense.bias\n\t bert_layer.encoder.layer.2.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.2.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.2.intermediate.dense.weight\n\t bert_layer.encoder.layer.2.intermediate.dense.bias\n\t bert_layer.encoder.layer.2.output.dense.weight\n\t bert_layer.encoder.layer.2.output.dense.bias\n\t bert_layer.encoder.layer.2.output.LayerNorm.weight\n\t bert_layer.encoder.layer.2.output.LayerNorm.bias\n\t bert_layer.encoder.layer.3.attention.self.query.weight\n\t bert_layer.encoder.layer.3.attention.self.query.bias\n\t bert_layer.encoder.layer.3.attention.self.key.weight\n\t bert_layer.encoder.layer.3.attention.self.key.bias\n\t bert_layer.encoder.layer.3.attention.self.value.weight\n\t bert_layer.encoder.layer.3.attention.self.value.bias\n\t bert_layer.encoder.layer.3.attention.output.dense.weight\n\t bert_layer.encoder.layer.3.attention.output.dense.bias\n\t bert_layer.encoder.layer.3.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.3.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.3.intermediate.dense.weight\n\t bert_layer.encoder.layer.3.intermediate.dense.bias\n\t bert_layer.encoder.layer.3.output.dense.weight\n\t bert_layer.encoder.layer.3.output.dense.bias\n\t bert_layer.encoder.layer.3.output.LayerNorm.weight\n\t bert_layer.encoder.layer.3.output.LayerNorm.bias\n\t bert_layer.encoder.layer.4.attention.self.query.weight\n\t bert_layer.encoder.layer.4.attention.self.query.bias\n\t bert_layer.encoder.layer.4.attention.self.key.weight\n\t bert_layer.encoder.layer.4.attention.self.key.bias\n\t bert_layer.encoder.layer.4.attention.self.value.weight\n\t bert_layer.encoder.layer.4.attention.self.value.bias\n\t bert_layer.encoder.layer.4.attention.output.dense.weight\n\t bert_layer.encoder.layer.4.attention.output.dense.bias\n\t bert_layer.encoder.layer.4.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.4.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.4.intermediate.dense.weight\n\t bert_layer.encoder.layer.4.intermediate.dense.bias\n\t bert_layer.encoder.layer.4.output.dense.weight\n\t bert_layer.encoder.layer.4.output.dense.bias\n\t bert_layer.encoder.layer.4.output.LayerNorm.weight\n\t bert_layer.encoder.layer.4.output.LayerNorm.bias\n\t bert_layer.encoder.layer.5.attention.self.query.weight\n\t bert_layer.encoder.layer.5.attention.self.query.bias\n\t bert_layer.encoder.layer.5.attention.self.key.weight\n\t bert_layer.encoder.layer.5.attention.self.key.bias\n\t bert_layer.encoder.layer.5.attention.self.value.weight\n\t bert_layer.encoder.layer.5.attention.self.value.bias\n\t bert_layer.encoder.layer.5.attention.output.dense.weight\n\t bert_layer.encoder.layer.5.attention.output.dense.bias\n\t bert_layer.encoder.layer.5.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.5.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.5.intermediate.dense.weight\n\t bert_layer.encoder.layer.5.intermediate.dense.bias\n\t bert_layer.encoder.layer.5.output.dense.weight\n\t bert_layer.encoder.layer.5.output.dense.bias\n\t bert_layer.encoder.layer.5.output.LayerNorm.weight\n\t bert_layer.encoder.layer.5.output.LayerNorm.bias\n\t bert_layer.encoder.layer.6.attention.self.query.weight\n\t bert_layer.encoder.layer.6.attention.self.query.bias\n\t bert_layer.encoder.layer.6.attention.self.key.weight\n\t bert_layer.encoder.layer.6.attention.self.key.bias\n\t bert_layer.encoder.layer.6.attention.self.value.weight\n\t bert_layer.encoder.layer.6.attention.self.value.bias\n\t bert_layer.encoder.layer.6.attention.output.dense.weight\n\t bert_layer.encoder.layer.6.attention.output.dense.bias\n\t bert_layer.encoder.layer.6.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.6.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.6.intermediate.dense.weight\n\t bert_layer.encoder.layer.6.intermediate.dense.bias\n\t bert_layer.encoder.layer.6.output.dense.weight\n\t bert_layer.encoder.layer.6.output.dense.bias\n\t bert_layer.encoder.layer.6.output.LayerNorm.weight\n\t bert_layer.encoder.layer.6.output.LayerNorm.bias\n\t bert_layer.encoder.layer.7.attention.self.query.weight\n\t bert_layer.encoder.layer.7.attention.self.query.bias\n\t bert_layer.encoder.layer.7.attention.self.key.weight\n\t bert_layer.encoder.layer.7.attention.self.key.bias\n\t bert_layer.encoder.layer.7.attention.self.value.weight\n\t bert_layer.encoder.layer.7.attention.self.value.bias\n\t bert_layer.encoder.layer.7.attention.output.dense.weight\n\t bert_layer.encoder.layer.7.attention.output.dense.bias\n\t bert_layer.encoder.layer.7.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.7.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.7.intermediate.dense.weight\n\t bert_layer.encoder.layer.7.intermediate.dense.bias\n\t bert_layer.encoder.layer.7.output.dense.weight\n\t bert_layer.encoder.layer.7.output.dense.bias\n\t bert_layer.encoder.layer.7.output.LayerNorm.weight\n\t bert_layer.encoder.layer.7.output.LayerNorm.bias\n\t bert_layer.encoder.layer.8.attention.self.query.weight\n\t bert_layer.encoder.layer.8.attention.self.query.bias\n\t bert_layer.encoder.layer.8.attention.self.key.weight\n\t bert_layer.encoder.layer.8.attention.self.key.bias\n\t bert_layer.encoder.layer.8.attention.self.value.weight\n\t bert_layer.encoder.layer.8.attention.self.value.bias\n\t bert_layer.encoder.layer.8.attention.output.dense.weight\n\t bert_layer.encoder.layer.8.attention.output.dense.bias\n\t bert_layer.encoder.layer.8.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.8.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.8.intermediate.dense.weight\n\t bert_layer.encoder.layer.8.intermediate.dense.bias\n\t bert_layer.encoder.layer.8.output.dense.weight\n\t bert_layer.encoder.layer.8.output.dense.bias\n\t bert_layer.encoder.layer.8.output.LayerNorm.weight\n\t bert_layer.encoder.layer.8.output.LayerNorm.bias\n\t bert_layer.encoder.layer.9.attention.self.query.weight\n\t bert_layer.encoder.layer.9.attention.self.query.bias\n\t bert_layer.encoder.layer.9.attention.self.key.weight\n\t bert_layer.encoder.layer.9.attention.self.key.bias\n\t bert_layer.encoder.layer.9.attention.self.value.weight\n\t bert_layer.encoder.layer.9.attention.self.value.bias\n\t bert_layer.encoder.layer.9.attention.output.dense.weight\n\t bert_layer.encoder.layer.9.attention.output.dense.bias\n\t bert_layer.encoder.layer.9.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.9.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.9.intermediate.dense.weight\n\t bert_layer.encoder.layer.9.intermediate.dense.bias\n\t bert_layer.encoder.layer.9.output.dense.weight\n\t bert_layer.encoder.layer.9.output.dense.bias\n\t bert_layer.encoder.layer.9.output.LayerNorm.weight\n\t bert_layer.encoder.layer.9.output.LayerNorm.bias\n\t bert_layer.encoder.layer.10.attention.self.query.weight\n\t bert_layer.encoder.layer.10.attention.self.query.bias\n\t bert_layer.encoder.layer.10.attention.self.key.weight\n\t bert_layer.encoder.layer.10.attention.self.key.bias\n\t bert_layer.encoder.layer.10.attention.self.value.weight\n\t bert_layer.encoder.layer.10.attention.self.value.bias\n\t bert_layer.encoder.layer.10.attention.output.dense.weight\n\t bert_layer.encoder.layer.10.attention.output.dense.bias\n\t bert_layer.encoder.layer.10.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.10.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.10.intermediate.dense.weight\n\t bert_layer.encoder.layer.10.intermediate.dense.bias\n\t bert_layer.encoder.layer.10.output.dense.weight\n\t bert_layer.encoder.layer.10.output.dense.bias\n\t bert_layer.encoder.layer.10.output.LayerNorm.weight\n\t bert_layer.encoder.layer.10.output.LayerNorm.bias\n\t bert_layer.encoder.layer.11.attention.self.query.weight\n\t bert_layer.encoder.layer.11.attention.self.query.bias\n\t bert_layer.encoder.layer.11.attention.self.key.weight\n\t bert_layer.encoder.layer.11.attention.self.key.bias\n\t bert_layer.encoder.layer.11.attention.self.value.weight\n\t bert_layer.encoder.layer.11.attention.self.value.bias\n\t bert_layer.encoder.layer.11.attention.output.dense.weight\n\t bert_layer.encoder.layer.11.attention.output.dense.bias\n\t bert_layer.encoder.layer.11.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.11.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.11.intermediate.dense.weight\n\t bert_layer.encoder.layer.11.intermediate.dense.bias\n\t bert_layer.encoder.layer.11.output.dense.weight\n\t bert_layer.encoder.layer.11.output.dense.bias\n\t bert_layer.encoder.layer.11.output.LayerNorm.weight\n\t bert_layer.encoder.layer.11.output.LayerNorm.bias\n\t bert_layer.pooler.dense.weight\n\t bert_layer.pooler.dense.bias\n\t attention_layer.weight.weight\n\t modeling_layer.weight_ih_l0\n\t modeling_layer.weight_hh_l0\n\t modeling_layer.bias_ih_l0\n\t modeling_layer.bias_hh_l0\n\t modeling_layer.weight_ih_l1\n\t modeling_layer.weight_hh_l1\n\t modeling_layer.bias_ih_l1\n\t modeling_layer.bias_hh_l1\n\t prediction_layer.pred_start.weight\n\t prediction_layer.pred_end.weight\n"
     ]
    }
   ],
   "source": [
    "parameters = model.parameters()\n",
    "print(\"Parameters to learn:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"\\t\", name)\n",
    "optimizer = optim.Adam(parameters, lr=5e-5)"
   ]
  },
  {
   "source": [
    "This part should be the definition of training process:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(logits_start, logits_end, threshold = 0.1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    logits_start, logits_end: torch.tensor() of shape [batch_size, sequence length]\n",
    "    return the index i,j such that i<=j and logits_start[i]+logits[j] is maximized\n",
    "    \"\"\"\n",
    "    # compute probability\n",
    "    p_start = F.softmax(logits_start, dim=-1)\n",
    "    p_end = F.softmax(logits_end, dim=-1)\n",
    "    # compute joint probability\n",
    "    p_joint = torch.triu(torch.bmm(p_start.unsqueeze(dim=2), p_end.unsqueeze(dim=1)))\n",
    "    # get the batchwise indices\n",
    "    max_row, _ = torch.max(p_joint, dim=2)\n",
    "    max_col, _ = torch.max(p_joint, dim=1)\n",
    "    start = torch.argmax(max_in_row, dim=-1)\n",
    "    end = torch.argmax(max_in_col, dim=-1)\n",
    "    # check if indices are greater than no answer probability by threshold\n",
    "    p_na = p_joint[:,0,0]\n",
    "    max_prob = torch.max(max_row,dim=-1)\n",
    "    start[p_na + threshold > max_prob] = 0\n",
    "    end[p_na + threshold > max_prob] = 0\n",
    "    # adjust to the encoding structure\n",
    "    start[start!=0] += 63\n",
    "    end[end!=0] += 63\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, num_epochs = 3):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    model: a pytorch model\n",
    "    dataloader: a pytorch dataloader\n",
    "    loss_func: a pytorch criterion, e.g. torch.nn.CrossEntropyLoss()\n",
    "    optimizer: an optimizer: e.g. torch.optim.SGD()\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1))\n",
    "        print('-'*10)\n",
    "        # Each epoch we make a training and a validation phase\n",
    "        model.train()\n",
    "            \n",
    "        # Initialize the loss and binary classification error in each epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Send data to GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "            # Forward computation\n",
    "            # Get the model outputs\n",
    "            outputs = model(input_ids, attention_mask, start_positions, end_positions)\n",
    "            loss = outputs[0]\n",
    "            # In training phase, backprop and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()                   \n",
    "            # Compute running loss/accuracy\n",
    "            running_loss += loss\n",
    "\n",
    "        epoch_loss = running_loss\n",
    "        print('Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "    # Output info after training\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "def word_tokenize(sent, nlp):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = word_tokenize(a_gold)\n",
    "    pred_toks = word_tokenize(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataset, answers, threshold=0.1):\n",
    "    \"\"\" TODO: debug\"\"\"\n",
    "    n = len(eval_dataset)\n",
    "    exact_match = 0\n",
    "    f1_sum = 0\n",
    "    model.eval()\n",
    "    for i in range(n):\n",
    "        input_ids = eval_dataset[i]['input_ids']\n",
    "        attention_mask = eval_dataset[i]['attention_mask']\n",
    "        golden_answer = answers[i]['text']\n",
    "\n",
    "        _, start_logits, end_logits = model(torch.unsqueeze(input_ids,0), torch.unsqueeze(attention_mask,0))\n",
    "\n",
    "        # compute null score and make prediction:\n",
    "        start, end = predict_index(start_logits, end_logits, threshold)\n",
    "        if start == 0 and end == 0:\n",
    "            prediction = \"\"\n",
    "        else:\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            prediction = ' '.join(tokens[start:end+1])\n",
    "        \n",
    "        #exact match\n",
    "        if(prediction == golden_answer):\n",
    "            exact_match = exact_match + 1\n",
    "        #F1_score\n",
    "        f1_sum = f1_sum + get_F1_score(golden_answer, prediction)       \n",
    "    accuracy = exact_match/n\n",
    "    f1 = f1_sum / n\n",
    "    return accuracy, f1"
   ]
  },
  {
   "source": [
    "Rest part is for experiments:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/29:\n",
      "----------\n",
      "Loss: 299.2440\n",
      "Epoch 1/29:\n",
      "----------\n",
      "Loss: 304.1715\n",
      "Epoch 2/29:\n",
      "----------\n",
      "Loss: 305.5480\n",
      "Epoch 3/29:\n",
      "----------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\nException raised from gemm at ..\\aten\\src\\ATen\\cuda\\CUDABlas.cpp:165 (most recent call first):\n00007FF8577075A200007FF857707540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFFE110384600007FFFE1102810 torch_cuda.dll!at::native::sparse_mask_cuda [<unknown file> @ <unknown line number>]\n00007FFFE060C89700007FFFE060B790 torch_cuda.dll!at::native::lerp_cuda_tensor_out [<unknown file> @ <unknown line number>]\n00007FFFE060E2D200007FFFE060DD60 torch_cuda.dll!at::native::addmm_out_cuda [<unknown file> @ <unknown line number>]\n00007FFFE060F44300007FFFE060F360 torch_cuda.dll!at::native::mm_cuda [<unknown file> @ <unknown line number>]\n00007FFFE1171E6F00007FFFE110E400 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFFE1161E8200007FFFE110E400 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF81B40D94900007FF81B408FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF81B44057700007FF81B440520 torch_cpu.dll!at::mm [<unknown file> @ <unknown line number>]\n00007FF81C79EC7900007FF81C6AE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FF81AF5715700007FF81AF56290 torch_cpu.dll!at::indexing::TensorIndex::boolean [<unknown file> @ <unknown line number>]\n00007FF81B40D94900007FF81B408FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF81B52210700007FF81B5220B0 torch_cpu.dll!at::Tensor::mm [<unknown file> @ <unknown line number>]\n00007FF81C611F1600007FF81C611B30 torch_cpu.dll!torch::autograd::generated::MmBackward::apply [<unknown file> @ <unknown line number>]\n00007FF81C5E7E9100007FF81C5E7B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FF81CB4F9BA00007FF81CB4F300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FF81CB503AD00007FF81CB4FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FF81CB54FE200007FF81CB54CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FF81CB54C4100007FF81CB54BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FF801AE0A2700007FF801ABA100 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FF81CB4BF1400007FF81CB4B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FF887FFE3FE00007FF887FFE3A0 ucrtbase.dll!o_strcat_s [<unknown file> @ <unknown line number>]\n00007FF88A08403400007FF88A084020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FF88BCB369100007FF88BCB3670 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-911f61890ff6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-c6b5f4bdbacf>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, dataloader, num_epochs)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# In training phase, backprop and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Compute running loss/accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\nException raised from gemm at ..\\aten\\src\\ATen\\cuda\\CUDABlas.cpp:165 (most recent call first):\n00007FF8577075A200007FF857707540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFFE110384600007FFFE1102810 torch_cuda.dll!at::native::sparse_mask_cuda [<unknown file> @ <unknown line number>]\n00007FFFE060C89700007FFFE060B790 torch_cuda.dll!at::native::lerp_cuda_tensor_out [<unknown file> @ <unknown line number>]\n00007FFFE060E2D200007FFFE060DD60 torch_cuda.dll!at::native::addmm_out_cuda [<unknown file> @ <unknown line number>]\n00007FFFE060F44300007FFFE060F360 torch_cuda.dll!at::native::mm_cuda [<unknown file> @ <unknown line number>]\n00007FFFE1171E6F00007FFFE110E400 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFFE1161E8200007FFFE110E400 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF81B40D94900007FF81B408FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF81B44057700007FF81B440520 torch_cpu.dll!at::mm [<unknown file> @ <unknown line number>]\n00007FF81C79EC7900007FF81C6AE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FF81AF5715700007FF81AF56290 torch_cpu.dll!at::indexing::TensorIndex::boolean [<unknown file> @ <unknown line number>]\n00007FF81B40D94900007FF81B408FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF81B52210700007FF81B5220B0 torch_cpu.dll!at::Tensor::mm [<unknown file> @ <unknown line number>]\n00007FF81C611F1600007FF81C611B30 torch_cpu.dll!torch::autograd::generated::MmBackward::apply [<unknown file> @ <unknown line number>]\n00007FF81C5E7E9100007FF81C5E7B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FF81CB4F9BA00007FF81CB4F300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FF81CB503AD00007FF81CB4FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FF81CB54FE200007FF81CB54CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FF81CB54C4100007FF81CB54BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FF801AE0A2700007FF801ABA100 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FF81CB4BF1400007FF81CB4B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FF887FFE3FE00007FF887FFE3A0 ucrtbase.dll!o_strcat_s [<unknown file> @ <unknown line number>]\n00007FF88A08403400007FF88A084020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FF88BCB369100007FF88BCB3670 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(model, optimizer, dataloader, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em, f1 = evaluate(trained_model, val_dataset, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2512487424"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}