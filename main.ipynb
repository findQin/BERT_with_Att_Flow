{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this notebook we train our model: a BERT-like model with attention flow inspired by BiDAF."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Dependencies:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import collections, time, spacy, copy\n",
    "from layers.bert_plus_bidaf import BERT_plus_BiDAF\n",
    "from utils import data_processing\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "source": [
    "This part should be data loading and processing.\n",
    "\n",
    "Input: SQuAD dataset handler/url/json\n",
    "\n",
    "Output: processed dict/list/whatever: train_question, train_context, train_answer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-64d003fa4bfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mval_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_encodings\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdata_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\OneDrive\\Courses\\ECS289 NLP\\BERT_with_Att_Flow\\utils\\data_processing.py\u001b[0m in \u001b[0;36mdata_processing\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0madd_token_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[0mpaddingLengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostTokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mmodify_token_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpaddingLengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\OneDrive\\Courses\\ECS289 NLP\\BERT_with_Att_Flow\\utils\\data_processing.py\u001b[0m in \u001b[0;36mmodify_token_positions\u001b[1;34m(encodings, paddingLengths, answers)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mend_positions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mstart_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_positions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpaddingLengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mend_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_positions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpaddingLengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart_position\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m511\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "val_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "train_encodings =  data_processing.data_processing(train_url)\n",
    "val_encodings = data_processing.data_processing(val_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,encodings):\n",
    "    self.encodings = encodings\n",
    "  def __getitem__(self,idx):\n",
    "    return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "source": [
    "This part should be model construction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_plus_BiDAF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BERT_plus_BiDAF(\n",
       "  (bert_layer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attention_layer): AttFlow(\n",
       "    (weight): Linear(in_features=2304, out_features=1, bias=False)\n",
       "  )\n",
       "  (prediction_layer): PredictionLayer(\n",
       "    (pred_start): Linear(in_features=3072, out_features=1, bias=False)\n",
       "    (pred_end): Linear(in_features=3072, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "source": [
    "This part should be declaration of the optimizer and the loss function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameters to learn:\n\t bert_layer.embeddings.word_embeddings.weight\n\t bert_layer.embeddings.position_embeddings.weight\n\t bert_layer.embeddings.token_type_embeddings.weight\n\t bert_layer.embeddings.LayerNorm.weight\n\t bert_layer.embeddings.LayerNorm.bias\n\t bert_layer.encoder.layer.0.attention.self.query.weight\n\t bert_layer.encoder.layer.0.attention.self.query.bias\n\t bert_layer.encoder.layer.0.attention.self.key.weight\n\t bert_layer.encoder.layer.0.attention.self.key.bias\n\t bert_layer.encoder.layer.0.attention.self.value.weight\n\t bert_layer.encoder.layer.0.attention.self.value.bias\n\t bert_layer.encoder.layer.0.attention.output.dense.weight\n\t bert_layer.encoder.layer.0.attention.output.dense.bias\n\t bert_layer.encoder.layer.0.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.0.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.0.intermediate.dense.weight\n\t bert_layer.encoder.layer.0.intermediate.dense.bias\n\t bert_layer.encoder.layer.0.output.dense.weight\n\t bert_layer.encoder.layer.0.output.dense.bias\n\t bert_layer.encoder.layer.0.output.LayerNorm.weight\n\t bert_layer.encoder.layer.0.output.LayerNorm.bias\n\t bert_layer.encoder.layer.1.attention.self.query.weight\n\t bert_layer.encoder.layer.1.attention.self.query.bias\n\t bert_layer.encoder.layer.1.attention.self.key.weight\n\t bert_layer.encoder.layer.1.attention.self.key.bias\n\t bert_layer.encoder.layer.1.attention.self.value.weight\n\t bert_layer.encoder.layer.1.attention.self.value.bias\n\t bert_layer.encoder.layer.1.attention.output.dense.weight\n\t bert_layer.encoder.layer.1.attention.output.dense.bias\n\t bert_layer.encoder.layer.1.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.1.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.1.intermediate.dense.weight\n\t bert_layer.encoder.layer.1.intermediate.dense.bias\n\t bert_layer.encoder.layer.1.output.dense.weight\n\t bert_layer.encoder.layer.1.output.dense.bias\n\t bert_layer.encoder.layer.1.output.LayerNorm.weight\n\t bert_layer.encoder.layer.1.output.LayerNorm.bias\n\t bert_layer.encoder.layer.2.attention.self.query.weight\n\t bert_layer.encoder.layer.2.attention.self.query.bias\n\t bert_layer.encoder.layer.2.attention.self.key.weight\n\t bert_layer.encoder.layer.2.attention.self.key.bias\n\t bert_layer.encoder.layer.2.attention.self.value.weight\n\t bert_layer.encoder.layer.2.attention.self.value.bias\n\t bert_layer.encoder.layer.2.attention.output.dense.weight\n\t bert_layer.encoder.layer.2.attention.output.dense.bias\n\t bert_layer.encoder.layer.2.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.2.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.2.intermediate.dense.weight\n\t bert_layer.encoder.layer.2.intermediate.dense.bias\n\t bert_layer.encoder.layer.2.output.dense.weight\n\t bert_layer.encoder.layer.2.output.dense.bias\n\t bert_layer.encoder.layer.2.output.LayerNorm.weight\n\t bert_layer.encoder.layer.2.output.LayerNorm.bias\n\t bert_layer.encoder.layer.3.attention.self.query.weight\n\t bert_layer.encoder.layer.3.attention.self.query.bias\n\t bert_layer.encoder.layer.3.attention.self.key.weight\n\t bert_layer.encoder.layer.3.attention.self.key.bias\n\t bert_layer.encoder.layer.3.attention.self.value.weight\n\t bert_layer.encoder.layer.3.attention.self.value.bias\n\t bert_layer.encoder.layer.3.attention.output.dense.weight\n\t bert_layer.encoder.layer.3.attention.output.dense.bias\n\t bert_layer.encoder.layer.3.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.3.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.3.intermediate.dense.weight\n\t bert_layer.encoder.layer.3.intermediate.dense.bias\n\t bert_layer.encoder.layer.3.output.dense.weight\n\t bert_layer.encoder.layer.3.output.dense.bias\n\t bert_layer.encoder.layer.3.output.LayerNorm.weight\n\t bert_layer.encoder.layer.3.output.LayerNorm.bias\n\t bert_layer.encoder.layer.4.attention.self.query.weight\n\t bert_layer.encoder.layer.4.attention.self.query.bias\n\t bert_layer.encoder.layer.4.attention.self.key.weight\n\t bert_layer.encoder.layer.4.attention.self.key.bias\n\t bert_layer.encoder.layer.4.attention.self.value.weight\n\t bert_layer.encoder.layer.4.attention.self.value.bias\n\t bert_layer.encoder.layer.4.attention.output.dense.weight\n\t bert_layer.encoder.layer.4.attention.output.dense.bias\n\t bert_layer.encoder.layer.4.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.4.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.4.intermediate.dense.weight\n\t bert_layer.encoder.layer.4.intermediate.dense.bias\n\t bert_layer.encoder.layer.4.output.dense.weight\n\t bert_layer.encoder.layer.4.output.dense.bias\n\t bert_layer.encoder.layer.4.output.LayerNorm.weight\n\t bert_layer.encoder.layer.4.output.LayerNorm.bias\n\t bert_layer.encoder.layer.5.attention.self.query.weight\n\t bert_layer.encoder.layer.5.attention.self.query.bias\n\t bert_layer.encoder.layer.5.attention.self.key.weight\n\t bert_layer.encoder.layer.5.attention.self.key.bias\n\t bert_layer.encoder.layer.5.attention.self.value.weight\n\t bert_layer.encoder.layer.5.attention.self.value.bias\n\t bert_layer.encoder.layer.5.attention.output.dense.weight\n\t bert_layer.encoder.layer.5.attention.output.dense.bias\n\t bert_layer.encoder.layer.5.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.5.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.5.intermediate.dense.weight\n\t bert_layer.encoder.layer.5.intermediate.dense.bias\n\t bert_layer.encoder.layer.5.output.dense.weight\n\t bert_layer.encoder.layer.5.output.dense.bias\n\t bert_layer.encoder.layer.5.output.LayerNorm.weight\n\t bert_layer.encoder.layer.5.output.LayerNorm.bias\n\t bert_layer.encoder.layer.6.attention.self.query.weight\n\t bert_layer.encoder.layer.6.attention.self.query.bias\n\t bert_layer.encoder.layer.6.attention.self.key.weight\n\t bert_layer.encoder.layer.6.attention.self.key.bias\n\t bert_layer.encoder.layer.6.attention.self.value.weight\n\t bert_layer.encoder.layer.6.attention.self.value.bias\n\t bert_layer.encoder.layer.6.attention.output.dense.weight\n\t bert_layer.encoder.layer.6.attention.output.dense.bias\n\t bert_layer.encoder.layer.6.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.6.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.6.intermediate.dense.weight\n\t bert_layer.encoder.layer.6.intermediate.dense.bias\n\t bert_layer.encoder.layer.6.output.dense.weight\n\t bert_layer.encoder.layer.6.output.dense.bias\n\t bert_layer.encoder.layer.6.output.LayerNorm.weight\n\t bert_layer.encoder.layer.6.output.LayerNorm.bias\n\t bert_layer.encoder.layer.7.attention.self.query.weight\n\t bert_layer.encoder.layer.7.attention.self.query.bias\n\t bert_layer.encoder.layer.7.attention.self.key.weight\n\t bert_layer.encoder.layer.7.attention.self.key.bias\n\t bert_layer.encoder.layer.7.attention.self.value.weight\n\t bert_layer.encoder.layer.7.attention.self.value.bias\n\t bert_layer.encoder.layer.7.attention.output.dense.weight\n\t bert_layer.encoder.layer.7.attention.output.dense.bias\n\t bert_layer.encoder.layer.7.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.7.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.7.intermediate.dense.weight\n\t bert_layer.encoder.layer.7.intermediate.dense.bias\n\t bert_layer.encoder.layer.7.output.dense.weight\n\t bert_layer.encoder.layer.7.output.dense.bias\n\t bert_layer.encoder.layer.7.output.LayerNorm.weight\n\t bert_layer.encoder.layer.7.output.LayerNorm.bias\n\t bert_layer.encoder.layer.8.attention.self.query.weight\n\t bert_layer.encoder.layer.8.attention.self.query.bias\n\t bert_layer.encoder.layer.8.attention.self.key.weight\n\t bert_layer.encoder.layer.8.attention.self.key.bias\n\t bert_layer.encoder.layer.8.attention.self.value.weight\n\t bert_layer.encoder.layer.8.attention.self.value.bias\n\t bert_layer.encoder.layer.8.attention.output.dense.weight\n\t bert_layer.encoder.layer.8.attention.output.dense.bias\n\t bert_layer.encoder.layer.8.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.8.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.8.intermediate.dense.weight\n\t bert_layer.encoder.layer.8.intermediate.dense.bias\n\t bert_layer.encoder.layer.8.output.dense.weight\n\t bert_layer.encoder.layer.8.output.dense.bias\n\t bert_layer.encoder.layer.8.output.LayerNorm.weight\n\t bert_layer.encoder.layer.8.output.LayerNorm.bias\n\t bert_layer.encoder.layer.9.attention.self.query.weight\n\t bert_layer.encoder.layer.9.attention.self.query.bias\n\t bert_layer.encoder.layer.9.attention.self.key.weight\n\t bert_layer.encoder.layer.9.attention.self.key.bias\n\t bert_layer.encoder.layer.9.attention.self.value.weight\n\t bert_layer.encoder.layer.9.attention.self.value.bias\n\t bert_layer.encoder.layer.9.attention.output.dense.weight\n\t bert_layer.encoder.layer.9.attention.output.dense.bias\n\t bert_layer.encoder.layer.9.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.9.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.9.intermediate.dense.weight\n\t bert_layer.encoder.layer.9.intermediate.dense.bias\n\t bert_layer.encoder.layer.9.output.dense.weight\n\t bert_layer.encoder.layer.9.output.dense.bias\n\t bert_layer.encoder.layer.9.output.LayerNorm.weight\n\t bert_layer.encoder.layer.9.output.LayerNorm.bias\n\t bert_layer.encoder.layer.10.attention.self.query.weight\n\t bert_layer.encoder.layer.10.attention.self.query.bias\n\t bert_layer.encoder.layer.10.attention.self.key.weight\n\t bert_layer.encoder.layer.10.attention.self.key.bias\n\t bert_layer.encoder.layer.10.attention.self.value.weight\n\t bert_layer.encoder.layer.10.attention.self.value.bias\n\t bert_layer.encoder.layer.10.attention.output.dense.weight\n\t bert_layer.encoder.layer.10.attention.output.dense.bias\n\t bert_layer.encoder.layer.10.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.10.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.10.intermediate.dense.weight\n\t bert_layer.encoder.layer.10.intermediate.dense.bias\n\t bert_layer.encoder.layer.10.output.dense.weight\n\t bert_layer.encoder.layer.10.output.dense.bias\n\t bert_layer.encoder.layer.10.output.LayerNorm.weight\n\t bert_layer.encoder.layer.10.output.LayerNorm.bias\n\t bert_layer.encoder.layer.11.attention.self.query.weight\n\t bert_layer.encoder.layer.11.attention.self.query.bias\n\t bert_layer.encoder.layer.11.attention.self.key.weight\n\t bert_layer.encoder.layer.11.attention.self.key.bias\n\t bert_layer.encoder.layer.11.attention.self.value.weight\n\t bert_layer.encoder.layer.11.attention.self.value.bias\n\t bert_layer.encoder.layer.11.attention.output.dense.weight\n\t bert_layer.encoder.layer.11.attention.output.dense.bias\n\t bert_layer.encoder.layer.11.attention.output.LayerNorm.weight\n\t bert_layer.encoder.layer.11.attention.output.LayerNorm.bias\n\t bert_layer.encoder.layer.11.intermediate.dense.weight\n\t bert_layer.encoder.layer.11.intermediate.dense.bias\n\t bert_layer.encoder.layer.11.output.dense.weight\n\t bert_layer.encoder.layer.11.output.dense.bias\n\t bert_layer.encoder.layer.11.output.LayerNorm.weight\n\t bert_layer.encoder.layer.11.output.LayerNorm.bias\n\t bert_layer.pooler.dense.weight\n\t bert_layer.pooler.dense.bias\n\t attention_layer.weight.weight\n\t prediction_layer.pred_start.weight\n\t prediction_layer.pred_end.weight\n"
     ]
    }
   ],
   "source": [
    "parameters = model.parameters()\n",
    "print(\"Parameters to learn:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"\\t\", name)\n",
    "optimizer = optim.Adam(parameters)"
   ]
  },
  {
   "source": [
    "This part should be the definition of training process:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(logits_start, logits_end, threshold = 0.1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    logits_start, logits_end: torch.tensor() of shape [batch_size, sequence length]\n",
    "    return the index i,j such that i<=j and logits_start[i]+logits[j] is maximized\n",
    "    \"\"\"\n",
    "    # compute probability\n",
    "    p_start = F.softmax(logits_start, dim=-1)\n",
    "    p_end = F.softmax(logits_end, dim=-1)\n",
    "    # compute joint probability\n",
    "    p_joint = torch.triu(torch.bmm(p_start.unsqueeze(dim=2), p_end.unsqueeze(dim=1)))\n",
    "    # get the batchwise indices\n",
    "    max_row, _ = torch.max(p_joint, dim=2)\n",
    "    max_col, _ = torch.max(p_joint, dim=1)\n",
    "    start = torch.argmax(max_in_row, dim=-1)\n",
    "    end = torch.argmax(max_in_col, dim=-1)\n",
    "    # check if indices are greater than no answer probability by threshold\n",
    "    p_na = p_joint[:,0,0]\n",
    "    max_prob = torch.max(max_row,dim=-1)\n",
    "    start[p_na + threshold > max_prob] = 0\n",
    "    end[p_na + threshold > max_prob] = 0\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, num_epochs = 3, learning_rate = 5e-5):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    model: a pytorch model\n",
    "    dataloader: a pytorch dataloader\n",
    "    loss_func: a pytorch criterion, e.g. torch.nn.CrossEntropyLoss()\n",
    "    optimizer: an optimizer: e.g. torch.optim.SGD()\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1))\n",
    "        print('-'*10)\n",
    "        # Each epoch we make a training and a validation phase\n",
    "        model.train()\n",
    "            \n",
    "        # Initialize the loss and binary classification error in each epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Send data to GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "            # Forward computation\n",
    "            # Get the model outputs\n",
    "            outputs = model(input_ids, attention_mask, start_positions, end_positions)\n",
    "            loss = outputs[0]\n",
    "            # In training phase, backprop and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()                   \n",
    "            # Compute running loss/accuracy\n",
    "            running_loss += loss\n",
    "\n",
    "        epoch_loss = running_loss\n",
    "        print('Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "    # Output info after training\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "def word_tokenize(sent, nlp):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = word_tokenize(a_gold)\n",
    "    pred_toks = word_tokenize(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataset, answers, threshold=0.1):\n",
    "    n = len(eval_dataset)\n",
    "    exact_match = 0\n",
    "    f1_sum = 0\n",
    "    model.eval()\n",
    "    for i in range(n):\n",
    "        input_ids = eval_dataset[i]['input_ids']\n",
    "        attention_mask = eval_dataset[i]['attention_mask']\n",
    "        golden_answer = answers[i]['text']\n",
    "\n",
    "        _, start_logits, end_logits = model(torch.unsqueeze(input_ids,0), torch.unsqueeze(attention_mask,0))\n",
    "\n",
    "        # compute null score and make prediction:\n",
    "        start, end = predict_index(start_logits, end_logits, threshold)\n",
    "        if start == 0 and end == 0:\n",
    "            prediction = \"\"\n",
    "        else:\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            prediction = ' '.join(tokens[start:end+1])\n",
    "        \n",
    "        #exact match\n",
    "        if(prediction == golden_answer):\n",
    "            exact_match = exact_match + 1\n",
    "        #F1_score\n",
    "        f1_sum = f1_sum + get_F1_score(golden_answer, prediction)       \n",
    "    accuracy = exact_match/n\n",
    "    f1 = f1_sum / n\n",
    "    return accuracy, f1"
   ]
  },
  {
   "source": [
    "Rest part is for experiments:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train(model, dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em, f1 = evalueate(trained_model, val_dataset, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}