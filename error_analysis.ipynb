{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections, time, spacy, copy, string, re, json, matplotlib\n",
    "from layers.bert_plus_bidaf import BERT_plus_BiDAF\n",
    "from eval_test import evaluate\n",
    "from utils import data_processing\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from training import SquadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = torch.load(r'D:\\OneDrive\\Courses\\ECS289 NLP\\val_encodings.pt')\n",
    "val_answer=torch.load(r'D:\\OneDrive\\Courses\\ECS289 NLP\\val_answer.pt')\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "nlp = spacy.blank(\"en\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "model = BERT_plus_BiDAF(if_bidirectional=True, if_extra_modeling=True)\n",
    "model.load_state_dict(torch.load(r'D:\\OneDrive\\Courses\\ECS289 NLP\\bert_bidaf_bidirectionalLSTM.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.load('pred_logits.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(logits_start, logits_end, threshold = 0.1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    logits_start, logits_end: torch.tensor() of shape [batch_size, sequence length]\n",
    "    return the index i,j such that i<=j and logits_start[i]+logits[j] is maximized\n",
    "    \"\"\"\n",
    "    # compute probability\n",
    "    p_start = F.softmax(logits_start, dim=-1)\n",
    "    p_end = F.softmax(logits_end, dim=-1)\n",
    "    # compute joint probability\n",
    "    p_joint = torch.triu(torch.bmm(p_start.unsqueeze(dim=2), p_end.unsqueeze(dim=1)))\n",
    "    # get the batchwise indices\n",
    "    max_row, _ = torch.max(p_joint, dim=2)\n",
    "    max_col, _ = torch.max(p_joint, dim=1)\n",
    "    start = torch.argmax(max_row, dim=-1)\n",
    "    end = torch.argmax(max_col, dim=-1)\n",
    "    # check if indices are greater than no answer probability by threshold\n",
    "    p_na = p_joint[:,0,0]\n",
    "    max_prob,_ = torch.max(max_row,dim=-1)\n",
    "    start[p_na + threshold > max_prob] = 0\n",
    "    end[p_na + threshold > max_prob] = 0\n",
    "    return start, end\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "def compare(logits, eval_dataset, tokenizer, nlp, threshold):\n",
    "    \"\"\" To compare the predictions and answers, run this function\"\"\"\n",
    "    n = len(eval_dataset)\n",
    "    exact_match = 0\n",
    "    incorrectIdx = []\n",
    "    for i in range(n):\n",
    "        if i%1000==0:\n",
    "            print('compared {}/{}:'.format(i, n))\n",
    "        input_ids = eval_dataset[i]['input_ids']\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        golden_start, golden_end = eval_dataset[i]['start_positions'], eval_dataset[i]['end_positions']\n",
    "        if golden_start == 0 and golden_end == 0:\n",
    "            golden_answer = \"noanswer\"\n",
    "        else:\n",
    "            golden_answer = normalize_answer(' '.join(tokens[golden_start:golden_end + 1]))\n",
    "        \n",
    "        start_logits, end_logits = logits[i]['start_logits'], logits[i]['end_logits']\n",
    "        # compute null score and make prediction:\n",
    "        pred_start, pred_end = predict(torch.unsqueeze(start_logits,dim=0),torch.unsqueeze(end_logits,dim=0), threshold)\n",
    "        # adjust to the context padding\n",
    "        pred_start[pred_start!=0] += 62\n",
    "        pred_end[pred_end!=0] += 62\n",
    "        if pred_start == 0 or pred_end == 0:\n",
    "                pred_answer = \"noanswer\"\n",
    "        else:\n",
    "            pred_answer = normalize_answer(' '.join(tokens[pred_start:pred_end + 1]))\n",
    "        if pred_answer == golden_answer:\n",
    "            exact_match += 1\n",
    "        else:\n",
    "            incorrectIdx.append(i)\n",
    "    acc = 100 * exact_match / n\n",
    "    return acc, incorrectIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compared 0/11873:\n",
      "compared 1000/11873:\n",
      "compared 2000/11873:\n",
      "compared 3000/11873:\n",
      "compared 4000/11873:\n",
      "compared 5000/11873:\n",
      "compared 6000/11873:\n",
      "compared 7000/11873:\n",
      "compared 8000/11873:\n",
      "compared 9000/11873:\n",
      "compared 10000/11873:\n",
      "compared 11000/11873:\n"
     ]
    }
   ],
   "source": [
    "acc, idx = compare(logits, val_dataset, tokenizer, nlp, threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_error = len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "61.65248884022572 4691\n"
     ]
    }
   ],
   "source": [
    "print(acc, num_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2332, 3764, 4499, 1724, 2693, 2526,  587, 3405, 2259, 2729, 3505, 3081,\n        2572, 4233, 4168, 2234, 2765,  182, 4138, 2265, 3192,  823, 1600, 3499,\n         556, 2013, 4511, 2521, 4433,  174, 3637,  985, 2774, 3503,  883, 2692,\n        1156, 1138,  956, 4471, 4010, 3569, 1373,  913, 1986, 4553, 3073, 4244,\n        1293, 4645])\n"
     ]
    }
   ],
   "source": [
    "samples = torch.randperm(num_error)[0:50]\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6003, 9663, 11453, 4350, 6919, 6568, 1450, 8809, 5726, 7000, 9024, 7872, 6685, 10851, 10670, 5668, 7097, 480, 10577, 5737, 8174, 2060, 3984, 9011, 1381, 5172, 11488, 6560, 11321, 454, 9317, 2478, 7129, 9022, 2245, 6918, 2788, 2755, 2412, 11417, 10286, 9156, 3395, 2329, 5061, 11571, 7857, 10872, 3151, 11780]\n"
     ]
    }
   ],
   "source": [
    "sampled_errors = [idx[i] for i in samples]\n",
    "print(sampled_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "val_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "response = urllib.request.urlopen(val_url)\n",
    "raw = pd.read_json(response)\n",
    "contexts, questions, answers, ids = data_processing.load_data(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for error_idx in sampled_errors:\n",
    "    error_data = {}\n",
    "    input_ids = val_dataset[error_idx]['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    start_logits, end_logits = logits[error_idx]['start_logits'], logits[error_idx]['end_logits']\n",
    "    pred_start, pred_end = predict(torch.unsqueeze(start_logits,dim=0),torch.unsqueeze(end_logits,dim=0), 0)\n",
    "    # adjust to the context padding\n",
    "    pred_start[pred_start!=0] += 62\n",
    "    pred_end[pred_end!=0] += 62\n",
    "    if pred_start == 0 and pred_end == 0:\n",
    "                pred_answer = \"noanswer\"\n",
    "    else:\n",
    "            pred_answer = normalize_answer(' '.join(tokens[pred_start:pred_end + 1]))\n",
    "    error_data['context'] = contexts[error_idx]\n",
    "    error_data['question'] = questions[error_idx]\n",
    "    error_data['answer'] = val_answer[error_idx]['text']\n",
    "    error_data['predict answer'] = pred_answer\n",
    "    error_data['answer span'] = (val_dataset[error_idx]['start_positions'].item(), val_dataset[error_idx]['end_positions'].item())\n",
    "    error_data['predict answer span'] = (pred_start.item(), pred_end.item())\n",
    "    errors.append(error_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('error.json', 'w') as output:\n",
    "    json.dump(errors, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10577\ncontext: One key figure in the plans for what would come to be known as American Empire, was a geographer named Isiah Bowman. Bowman was the director of the American Geographical Society in 1914. Three years later in 1917, he was appointed to then President Woodrow Wilson's inquiry in 1917. The inquiry was the idea of President Wilson and the American delegation from the Paris Peace Conference. The point of this inquiry was to build a premise that would allow for U.S authorship of a 'new world' which was to be characterized by geographical order. As a result of his role in the inquiry, Isiah Bowman would come to be known as Wilson's geographer. \nquestion: What was the premise of Woodrow Wilson's inquiry?\nanswer: U.S authorship of a 'new world'\npredict answer: noanswer\nanswer span: (153, 162)\npredict answer span: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "index = 18\n",
    "print(sampled_errors[index])\n",
    "for key in errors[index].keys():\n",
    "    print(key+':', errors[index][key])"
   ]
  }
 ]
}